---
title: "Node Clustering"
publishedAt: '2025-03-22'
summary: "How to implement clustering with a Node server"
live: "false"
---


## Node's Event Loop is Single-Threaded

**The Problem:** Node's event loop is single-threaded, presenting the potential for blocking operations 
that can delay the processing of other I/O operations.

The diagram below is effectively the request/response cycle in an Express application. A request is made to a node server 
running in a single thread, which process that request and then sends a response. For smaller apps, this is a 
perfectly valid setup. 

{/* https://mermaid.js.org/syntax/flowchart.html#direction-in-subgraphs */}
<Mermaid className="w-full">
{`---
config: 
    theme: neutral
    look: handDrawn
---
flowchart LR;
    A[Request]
    B[Node Server]
    C[Response]
    A-->B
    B-->C`}
</Mermaid>

But, what if we have requests that require large amounts of processing power? Or many requests coming at a rapid pace?

The Node server will not be able to process incoming requests as fast as we would like in order for our app to be considered 
highly performant.  

To demonstrate this, I recommend running the example below inside of a new express app. You should only need to install the express library in order to run the code. If you're unable to run this code 
on your own, you can trust that I am not misrepresenting how the request/response cycle works.

```
// index.js 

const express = require('express');

const app = express();

function doWork(duration) {
    const start = Date.now();
    while (Date.now() - start < duration) {
        /* While the current time, less start time, is less 
             than duration, execute this loop's body.
           While this loop is being executed, the main thread is 
             blocked, meaning no other actions can be performed
             by our app. */
    }
}

app.get('/', (req, res) => {
    doWork(5000); // doWork will take 5 seconds to complete
    res.send('Hello');
})

app.listen(3001); // Any port you prefer here is fine
```

Start the server by running <code>node \<filename\></code> in your terminal. In your browser, 
open a tab to <span className="font-medium">localhost:3001</span>. 
If you chose a different port, replace 3001 with that port. 

Notice that no response was received from the server for ~5 seconds. 
This is due to the call to doWork taking 5 seconds to complete. 
While waiting for the function to complete, no other incoming requests could be processed. 
If you're using Chrome or a Chromium based browser, you can confirm this in your browser's Network tab.

<div className="relative w-full h-80">
<Image 
    fill
    alt="placeholder" 
    src="/evt-loop-blocked1.png"
/>
</div>
    {/* className="border border-neutral-300" */}


To simulate a request coming in while waiting for doWork to complete in the first request, open a second tab 
in your browser. Once both tabs have received responses, try to refresh one tab, then navigate to the other tab 
as fast as you can, and refresh it as well. 

On the tab you refreshed first, you should see it takes roughly the same amount of time as the prior example. 
The request sent from the second tab could not start to be processed until processing of the first request completed, 
so it will take longer to resolve.  It took me ~1.5 seconds after refreshing the first tab, to switch tabs and refresh 
the second tab, so the second request should start processing ~3.5 seconds after it was sent. The second request is 
identical to the first request, but since we had to wait ~3.5 seconds for the first request to complete, it took 
~8.5 seconds to complete.

## Clustering

Most modern computers are capable of handling multiple threads. For example,  y current laptop 
(MacBook Pro w/ M2 Pro chip) can run up to 12 threads at one time. 
Clustering allows us to run multiple instances of our app, with each instance occupying a single thread. 
How do we take advantage of all the extra threads our machines.

<Mermaid>
{`
---
config: 
    theme: neutral 
    look: handDrawn
---
flowchart LR
    Manager[Cluster Manager]
    A[Node Server A]
    B[Node Server B]
    C[Node Server C]

    Manager-->A
    Manager-->B
    Manager-->C
`}
</Mermaid>

The cluster manager is responsible for monitoring the health of each individual instance 
of our app that will be launched simultaneously.  It does not execute any code or handle any requests, 
data fetching etc. It simply monitors instance health, starts, stops and restarts each instance and 
can in some scenarios send data to each instance.  The instances themselves are still responsible for 
everything involved in processing requests.
{/* can in some scenarios send data (like environment variables) to each instance. */}


#### Traditional Flow

<Mermaid className="w-fit mx-auto">
{`
---
config: 
    theme: neutral 
    look: handDrawn
---
flowchart TD
    Run@{ shape: text, label: RUN node index.js }
    Index[index.js]
    Instance[Node Instance]

    Run-->Index
    Index-->Instance
`}
</Mermaid>

The traditional flow, without clustering, is shown by the diagram below. 
We start by running some command to call our main file, typically named *index.js*. 
Node takes the contents of that file, executes it and then enters into the event loop.


#### Cluster Flow
<Mermaid className="w-full mx-auto mt-4">
{`
---
config: 
    theme: neutral 
    look: handDrawn
---
flowchart TD
    Instance[Worker Instance]

    subgraph one [" "]
        Run@{ shape: text, label: RUN node index.js }
        Index[index.js]
        Manager[Cluster Manager]
        Fork@{ shape: text, label: cluster.fork() }

        Run-->Index
        Index-->Manager
        Manager-->Fork
        Fork-->Index
        end
    
    Index-->Instance

    style one stroke:transparent
`}
</Mermaid>

The first instance of node that gets launched is the cluster manager. The cluster manager is then responsible 
for starting each worker instance. Each worker instance will handle the processing of requests. 

One function in particular in the cluster manager, named *fork*, Node internally goes back to index.js and 
executes it again, in a slightly different mode, which we will get into further detail on later. For now, just 
know that every time index.js is executed **after** the first time, it will start a worker instance.

Recall the code example from earlier in this article that demonstrated event loop blocking. 
The code example below is similar, but now with three worker instances instead of a single app 
instance, and a '/fast' route was added, which skips a call to the doWork function.

In the traditional, non-cluster setup, if we called '/', which takes 5 seconds, and immediately after 
making the call, we called the '/fast' route, the request to '/fast' would not begin processing until 
after the work done by the request to '/' completes.

Now, with three instances of our app, there will be two idle instances waiting for requests while one instance waits 
for the call to doWork() to complete. This means that our call to '/fast' should return almost instantly (a few ms), even 
if a call to '/' had just been sent immediately before the request to '/fast'.

```
const cluster = require("cluster");

// cluster.isPrimary === true ONLY the first time this file is executed
console.log("\nIs Primary?", cluster.isPrimary);

// Is the file being executed in primary (aka `cluster manager`) mode?
if (cluster.isPrimary) {
  // If it is, cause index.js to be executed again, but
  // in child (aka `worker instance`) mode
  cluster.fork();
  // We can call cluster.fork() once for each worker instance 
  // we want created
  cluster.fork();
  cluster.fork();
  // Three calls to cluster.fork() means three worker instances 
  // will be created
} else {
  // Server related code should go here.
  const express = require("express");
  const app = express();

  function doWork(duration) {
    const start = Date.now();
    while (Date.now() - start < duration) {
      /* While the current time, less start time, is less 
             than duration, execute this loop's body.
           While this loop is being executed, the main thread is 
             blocked, meaning no other actions can be performed
             by our app. */
    }
  }

  app.get("/", (req, res) => {
    doWork(5000); // doWork will take 5 seconds to complete
    res.send("Hello");
  });

  app.get("/fast", (req, res) => {
    // Same as '/', except there will be no 5 second delay
    res.send("This was fast");
  });

  const PORT = 3001; // Any port you prefer here is fine
  app.listen(PORT, (err) => {
    if (!err) {
      console.log("\nServer Listening on Port", PORT, "\n");
    } else {
      console.error("\nServer failed to start:", err, "\n");
    }
  });
}
```

Again, I recommend running this in your browser.  Open two tabs, one to '/', and one to '/fast'.
Once both tabs have received responses, refresh the tab showing the '/' route, then as fast as possible, 
switch to the tab showing '/fast' and refresh it.  You should see you still get an immediate response from '/fast', 
even though the call to '/' has not yet responded.

In reality, the call to '/fast' takes ~10ms to complete, but to fit this nicely on a chart let's say it takes 500ms to complete. 
The call to '/' takes ~5s to complete.

The traditional flow would cause the call to '/fast' not to be process until after the call to '/' completes. 


<Mermaid className=''>
{`
---
config: 
    theme: neutral 
    look: handDrawn
    ganttConfig:
        barHeight: 50
---
gantt
    dateFormat X
    axisFormat %s

    section /
    5000    : 0, 5000
    section /fast
    500     : 5000, 5500
`}
</Mermaid>

With a cluster, there is another (maybe many more) instance available to handle the second 
request ('/fast'), so processing can begin as soon as the server receives the request.
<Mermaid className=''>
{`
---
config: 
    theme: neutral 
    look: handDrawn
    ganttConfig:
        barHeight: 50
---
gantt
    dateFormat X
    axisFormat %s

    section /
    5000    : 0, 5000

    section /fast
    500     :m4, 1000, 1500
`}
</Mermaid>